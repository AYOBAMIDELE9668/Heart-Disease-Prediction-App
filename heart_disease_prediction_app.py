# -*- coding: utf-8 -*-
"""Heart Disease prediction App

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ub8S-advmHcpNgz1sACgtcQHZV42HhWe

Install and Import Required Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files

"""Load the Dataset"""

df = pd.read_csv('heart_disease_uci.csv')

"""Display first few rows"""

print(df.head(5))

"""Inspect Data"""

print("\nDataset Info:")
print(df.info())
print("\nMissing Values:")
print(df.isnull().sum())

"""Rename target column and clean"""

df.rename(columns={'num': 'target'}, inplace=True)
df['target'] = (df['target'] > 0).astype(int)

"""Drop 'id' column (not useful)"""

df.drop(columns=['id'], inplace=True)

"""Handle Missing Values"""

df.replace({'': np.nan, '?': np.nan}, inplace=True)

"""Convert numeric columns to float where necessary"""

numeric_cols = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak', 'ca']
for col in numeric_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce')

"""Fill missing numeric values with median"""

for col in numeric_cols:
    df[col].fillna(df[col].median(), inplace=True)

"""Categorical columns: fill with mode"""

categorical_cols = ['cp', 'fbs', 'restecg', 'exang', 'slope', 'thal', 'sex', 'dataset']
for col in categorical_cols:
    df[col].fillna(df[col].mode()[0], inplace=True)

"""Encode Categorical Variables"""

categorical_cols.remove('sex')  # We'll handle 'sex' separately as binary
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

"""Encode 'sex': Male = 1, Female = 0"""

df['sex'] = df['sex'].map({'Male': 1, 'Female': 0})

print("Shape after encoding:", df.shape)

"""Define Features and Target"""

X = df.drop('target', axis=1)
y = df['target']

"""Train-Test Split"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

""" Scale the Features"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""Train Models"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve

"""Dictionary to store results"""

models = {}
results = []

"""Logistic Regression"""

lr = LogisticRegression(max_iter=1000, random_state=42)
lr.fit(X_train_scaled, y_train)
y_pred_lr = lr.predict(X_test_scaled)
y_prob_lr = lr.predict_proba(X_test_scaled)[:, 1]
models['Logistic Regression'] = lr
results.append({
    'Model': 'Logistic Regression',
    'Accuracy': accuracy_score(y_test, y_pred_lr),
    'Precision': precision_score(y_test, y_pred_lr),
    'Recall': recall_score(y_test, y_pred_lr),
    'F1': f1_score(y_test, y_pred_lr),
    'ROC-AUC': roc_auc_score(y_test, y_prob_lr)
})

"""Decision Tree"""

dt = DecisionTreeClassifier(random_state=42, max_depth=8, min_samples_split=5)
dt.fit(X_train, y_train)  # No scaling needed
y_pred_dt = dt.predict(X_test)
y_prob_dt = dt.predict_proba(X_test)[:, 1]
models['Decision Tree'] = dt
results.append({
    'Model': 'Decision Tree',
    'Accuracy': accuracy_score(y_test, y_pred_dt),
    'Precision': precision_score(y_test, y_pred_dt),
    'Recall': recall_score(y_test, y_pred_dt),
    'F1': f1_score(y_test, y_pred_dt),
    'ROC-AUC': roc_auc_score(y_test, y_prob_dt)
})

"""Random Forest"""

rf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_split=5,
                           random_state=42, class_weight='balanced')
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
y_prob_rf = rf.predict_proba(X_test)[:, 1]
models['Random Forest'] = rf
results.append({
    'Model': 'Random Forest',
    'Accuracy': accuracy_score(y_test, y_pred_rf),
    'Precision': precision_score(y_test, y_pred_rf),
    'Recall': recall_score(y_test, y_pred_rf),
    'F1': f1_score(y_test, y_pred_rf),
    'ROC-AUC': roc_auc_score(y_test, y_prob_rf)
})

"""Display Results"""

results_df = pd.DataFrame(results)
print("\n Model Performance Comparison:")
print(results_df.round(4))

"""Highlight best model"""

best_model = results_df.loc[results_df['F1'].idxmax()]['Model']
print(f"\n Best Model (by F1-score): {best_model}")

"""Confusion Matrix for Best Model"""

def plot_confusion_matrix(model_name):
    y_pred = models[model_name].predict(X_test if 'Tree' in model_name else X_test_scaled)
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No HD', 'HD'], yticklabels=['No HD', 'HD'])
    plt.title(f'Confusion Matrix - {model_name}')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

plot_confusion_matrix(best_model)

"""ROC Curve for All Models"""

plt.figure(figsize=(8,6))
for model_name in models.keys():
    if model_name == 'Logistic Regression':
        y_prob = lr.predict_proba(X_test_scaled)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        plt.plot(fpr, tpr, label=f"{model_name} (AUC = {roc_auc_score(y_test, y_prob):.3f})")
    elif model_name == 'Decision Tree':
        y_prob = dt.predict_proba(X_test)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        plt.plot(fpr, tpr, label=f"{model_name} (AUC = {roc_auc_score(y_test, y_prob):.3f})")
    elif model_name == 'Random Forest':
        y_prob = rf.predict_proba(X_test)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        plt.plot(fpr, tpr, label=f"{model_name} (AUC = {roc_auc_score(y_test, y_prob):.3f})")

plt.plot([0,1], [0,1], 'k--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Model Comparison')
plt.legend()
plt.grid(True)
plt.show()

"""Feature Importance (Random Forest)"""

if best_model == 'Random Forest':
    feat_importance = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
    plt.figure(figsize=(8,6))
    feat_importance.head(10).plot(kind='bar', color='skyblue')
    plt.title('Top 10 Feature Importances (Random Forest)')
    plt.ylabel('Importance')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

"""save model"""

import joblib

# Select best model
best_model_instance = models[best_model]

# Save model and scaler
joblib.dump(best_model_instance, 'heart_disease_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(X.columns.tolist(), 'feature_columns.pkl')  # Save feature names for inference

print("\nâœ… Best model and preprocessing objects saved!")
print("Files: 'heart_disease_model.pkl', 'scaler.pkl', 'feature_columns.pkl'")